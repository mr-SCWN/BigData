{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292",
   "metadata": {},
   "source": [
    "# Projekt Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480",
   "metadata": {},
   "source": [
    "# Wprowadzenie\n",
    "\n",
    "Wykorzystując ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n",
    "\n",
    "Kilka uwag:\n",
    "\n",
    "* Nie modyfikuj ani nie usuwaj paragrafów *markdown* w tym notatniku, chyba że wynika to jednoznacznie z instrukcji. \n",
    "* Istniejące paragrafy zawierające *kod* uzupełnij w razie potrzeby zgodnie z instrukcjami\n",
    "    - nie usuwaj ich\n",
    "    - nie usuwaj zawartych w nich instrukcji oraz kodu\n",
    "    - nie modyfikuj ich, jeśli instrukcje jawnie tego nie nakazują\n",
    "* Możesz dodawać nowe paragrafy zarówno zawierające kod jak i komentarze dotyczące tego kodu (markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c",
   "metadata": {},
   "source": [
    "# Treść projektu\n",
    "\n",
    "Poniżej w paragrafie markdown wstaw tytuł przydzielonego zestawu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16",
   "metadata": {},
   "source": [
    "# Zestaw 4 - imdb-persons\n",
    "\n",
    "**Uwaga**\n",
    "\n",
    "- W ramach wzorca nie są spełnione żadne reguły projektu. \n",
    "- Brak konsekwencji w wykorzystaniu właściwego API w ramach poszczególnych części\n",
    "- Zadanie *misji głównej* polega na zliczeniu słówek.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e128e43-6cce-4ffa-9609-9fae4b164ae9",
   "metadata": {},
   "source": [
    "# Działania wstępne \n",
    "\n",
    "Uruchom poniższy paragraf, aby utworzyć obiekty kontekstu Sparka. Jeśli jest taka potrzeba dostosuj te polecenia. Pamiętaj o potrzebnych bibliotekach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "26fb1050-386f-4398-ba5a-b45f5065d87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695a354-52bc-4bba-8222-7121bf07ae90",
   "metadata": {},
   "source": [
    "W poniższym paragrafie uzupełnij polecenia definiujące poszczególne zmienne. \n",
    "\n",
    "Pamiętaj abyś:\n",
    "\n",
    "* w późniejszym kodzie, dla wszystkich cześci projektu, korzystał z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n",
    "* przed ostateczną rejestracją projektu usunął ich wartości, tak aby nie pozostawiać w notatniku niczego co mogłoby identyfikować Ciebie jako jego autora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e883af01-7117-4faa-a840-7ff807a195d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pełna ścieżka do katalogu w zasobniku zawierającego podkatalogi `datasource1` i `datasource4` \n",
    "# z danymi źródłowymi\n",
    "input_dir = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4601cc7a-3ed5-47e2-994f-ebec642049b5",
   "metadata": {},
   "source": [
    "Nie modyfikuj poniższych paragrafów. Wykonaj je i używaj zdefniowanych poniżej zmiennych jak parametrów Twojego programu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6167e297-01ed-463e-bb81-9104d7cf7093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# ścieżki dla danych źródłowych \n",
    "datasource1_dir = input_dir + \"/datasource1\"\n",
    "datasource4_dir = input_dir + \"/datasource4\"\n",
    "\n",
    "# nazwy i ścieżki dla wyników dla misji głównej \n",
    "# część 1 (Spark Core - RDD) \n",
    "rdd_result_dir = \"/tmp/output1\"\n",
    "\n",
    "# część 2 (Spark SQL - DataFrame)\n",
    "df_result_table = \"output2\"\n",
    "\n",
    "# część 3 (Pandas API on Spark)\n",
    "ps_result_file = \"/tmp/output3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e36e0314-a4ac-4096-9e4b-23fd4a73e0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "import os\n",
    "def remove_file(file):\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "remove_file(\"metric_functions.py\")\n",
    "remove_file(\"tools_functions.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b4b8e00-10ae-47dc-b623-d1dacbe9c86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3322"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "import requests\n",
    "r = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\n",
    "open('metric_functions.py', 'wb').write(r.content)\n",
    "r = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\n",
    "open('tools_functions.py', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a433894-dc97-46f2-be51-9f40fa36894f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "%run metric_functions.py\n",
    "%run tools_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3a9dc-ac3b-4316-abb9-365caa1d7185",
   "metadata": {},
   "source": [
    "Poniższe paragrafy mają na celu usunąć ewentualne pozostałości poprzednich uruchomień tego lub innych notatników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08091c72-937f-41c2-9afe-d1505862bf1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `/tmp/output1': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error deleting file /tmp/output1: Command '['hadoop', 'fs', '-rm', '-r', '/tmp/output1']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 1 (Spark Core - RDD) \n",
    "delete_dir(spark, rdd_result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3e863c0-c824-47bd-b53a-ce3b1fd6d453",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table output2 does not exist.\n",
      "Error deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: `file:/spark-warehouse/output2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 2 (Spark SQL - DataFrame) \n",
    "drop_table(spark, df_result_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72956a1a-da48-4d2b-a07a-e03d56431d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "# usunięcie miejsca docelowego dla część 3 (Pandas API on Spark) \n",
    "remove_file(ps_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9e423d4-92b8-4161-98da-1a867f86d780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pbd-cluster-m.europe-west4-c.c.big-data-2024-10-ik.internal:39999\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f79b2975c10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14faf05b-6c52-4b02-b2e5-2ddb3f38c704",
   "metadata": {},
   "source": [
    "***Uwaga!***\n",
    "\n",
    "Uruchom poniższy paragraf i sprawdź czy adres, pod którym dostępny *Apache Spark Application UI* jest poprawny wywołując następny testowy paragraf. \n",
    "\n",
    "W razie potrzeby określ samodzielnie poprawny adres, pod którym dostępny *Apache Spark Application UI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://pbd-cluster-m.europe-west4-c.c.big-data-2024-10-ik.internal:39999'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adres URL, pod którym dostępny Apache Spark Application UI (REST API)\n",
    "# \n",
    "spark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\n",
    "spark_ui_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32c2329e-1d7a-465f-a23b-333f95bf7deb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numTasks': 1398,\n",
       " 'numActiveTasks': 0,\n",
       " 'numCompleteTasks': 941,\n",
       " 'numFailedTasks': 0,\n",
       " 'numKilledTasks': 0,\n",
       " 'numCompletedIndices': 941,\n",
       " 'executorDeserializeTime': 34363,\n",
       " 'executorDeserializeCpuTime': 14622609865,\n",
       " 'executorRunTime': 3617406,\n",
       " 'executorCpuTime': 1007061952030,\n",
       " 'resultSize': 17031072,\n",
       " 'jvmGcTime': 24875,\n",
       " 'resultSerializationTime': 1649,\n",
       " 'memoryBytesSpilled': 0,\n",
       " 'diskBytesSpilled': 0,\n",
       " 'peakExecutionMemory': 29350619424,\n",
       " 'inputBytes': 18714469889,\n",
       " 'inputRecords': 372894753,\n",
       " 'outputBytes': 0,\n",
       " 'outputRecords': 0,\n",
       " 'shuffleRemoteBlocksFetched': 16932,\n",
       " 'shuffleLocalBlocksFetched': 16935,\n",
       " 'shuffleFetchWaitTime': 404,\n",
       " 'shuffleRemoteBytesRead': 1478502524,\n",
       " 'shuffleRemoteBytesReadToDisk': 0,\n",
       " 'shuffleLocalBytesRead': 4358440696,\n",
       " 'shuffleReadBytes': 5836943220,\n",
       " 'shuffleReadRecords': 142370648,\n",
       " 'shuffleWriteBytes': 4742577417,\n",
       " 'shuffleWriteTime': 10449458328,\n",
       " 'shuffleWriteRecords': 147174184}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testowy paragraf\n",
    "test_metrics = get_current_metrics(spark_ui_address)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccca69-c577-440c-aa5c-c9df3a54e127",
   "metadata": {},
   "source": [
    "# Część 1 - Spark Core (RDD)\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0af3440-983a-4cac-a8e7-4908b010947c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki Misja 1 (RDD):\n",
      "age | persons | actors | directors\n",
      "0\t7\t2\t0\n",
      "1\t5\t1\t0\n",
      "2\t3\t1\t0\n",
      "3\t5\t1\t0\n",
      "4\t6\t2\t0\n",
      "5\t4\t2\t0\n",
      "6\t11\t3\t0\n",
      "7\t10\t4\t0\n",
      "8\t7\t4\t0\n",
      "9\t10\t4\t0\n",
      "10\t17\t10\t0\n",
      "11\t15\t11\t0\n",
      "12\t16\t8\t0\n",
      "13\t22\t15\t0\n",
      "14\t38\t15\t0\n",
      "15\t32\t15\t1\n",
      "16\t41\t26\t1\n",
      "17\t50\t29\t2\n",
      "18\t71\t38\t1\n",
      "19\t86\t53\t1\n",
      "20\t115\t79\t2\n",
      "21\t103\t73\t2\n",
      "22\t149\t100\t3\n",
      "23\t160\t87\t5\n",
      "24\t199\t116\t9\n",
      "25\t219\t136\t4\n",
      "26\t265\t153\t12\n",
      "27\t304\t185\t7\n",
      "28\t343\t206\t15\n",
      "29\t352\t207\t18\n",
      "30\t390\t235\t23\n",
      "31\t408\t259\t31\n",
      "32\t421\t271\t31\n",
      "33\t438\t271\t29\n",
      "34\t558\t354\t31\n",
      "35\t515\t304\t47\n",
      "36\t547\t341\t53\n",
      "37\t622\t366\t53\n",
      "38\t645\t368\t61\n",
      "39\t721\t436\t61\n",
      "40\t746\t448\t72\n",
      "41\t776\t454\t91\n",
      "42\t754\t449\t78\n",
      "43\t867\t526\t100\n",
      "44\t937\t595\t92\n",
      "45\t989\t572\t106\n",
      "46\t1099\t643\t123\n",
      "47\t1082\t628\t118\n",
      "48\t1240\t720\t168\n",
      "49\t1338\t796\t142\n",
      "50\t1458\t839\t185\n",
      "51\t1462\t858\t181\n",
      "52\t1577\t961\t163\n",
      "53\t1675\t953\t189\n",
      "54\t1746\t1022\t201\n",
      "55\t1845\t1109\t202\n",
      "56\t1973\t1158\t229\n",
      "57\t2072\t1213\t234\n",
      "58\t2275\t1334\t273\n",
      "59\t2354\t1364\t258\n",
      "60\t2420\t1400\t271\n",
      "61\t2603\t1563\t280\n",
      "62\t2670\t1561\t299\n",
      "63\t2862\t1689\t329\n",
      "64\t2928\t1727\t332\n",
      "65\t3113\t1867\t370\n",
      "66\t3230\t1909\t347\n",
      "67\t3419\t2031\t375\n",
      "68\t3482\t2050\t404\n",
      "69\t3623\t2148\t439\n",
      "70\t3720\t2254\t403\n",
      "71\t3743\t2228\t415\n",
      "72\t3870\t2330\t440\n",
      "73\t3937\t2332\t451\n",
      "74\t4140\t2469\t447\n",
      "75\t4308\t2507\t450\n",
      "76\t4138\t2421\t458\n",
      "77\t4154\t2394\t476\n",
      "78\t4271\t2536\t454\n",
      "79\t4134\t2390\t462\n",
      "80\t4234\t2484\t445\n",
      "81\t4213\t2461\t440\n",
      "82\t4214\t2425\t428\n",
      "83\t4109\t2318\t417\n",
      "84\t4007\t2271\t437\n",
      "85\t3896\t2210\t413\n",
      "86\t3764\t2081\t406\n",
      "87\t3486\t1967\t362\n",
      "88\t3342\t1827\t322\n",
      "89\t3124\t1671\t305\n",
      "90\t2867\t1542\t290\n",
      "91\t2438\t1309\t209\n",
      "92\t2235\t1206\t184\n",
      "93\t1911\t1006\t162\n",
      "94\t1587\t826\t130\n",
      "95\t1316\t648\t109\n",
      "96\t1075\t546\t78\n",
      "97\t820\t374\t57\n",
      "98\t618\t306\t54\n",
      "99\t467\t206\t28\n",
      "100\t316\t152\t20\n",
      "101\t268\t125\t20\n",
      "102\t179\t79\t11\n",
      "103\t121\t57\t2\n",
      "104\t87\t34\t6\n",
      "105\t51\t19\t3\n",
      "106\t46\t19\t3\n",
      "107\t18\t4\t1\n",
      "108\t21\t4\t2\n",
      "109\t7\t2\t0\n",
      "110\t7\t2\t0\n",
      "111\t5\t1\t0\n",
      "112\t3\t1\t0\n",
      "113\t2\t0\t0\n",
      "114\t1\t0\t0\n",
      "115\t3\t0\t0\n",
      "116\t2\t0\t0\n",
      "117\t1\t1\t0\n",
      "118\t1\t0\t0\n",
      "121\t1\t1\t0\n",
      "122\t1\t0\t0\n"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 1 - RDD)\n",
    "#------------\n",
    "# Zadanie:\n",
    "# \"Przeanalizuj dane dotyczące zmarłych osób: ile osób żyło określoną liczbę lat (age), \n",
    "#  ilu z nich było aktorami (actors, actress, actors, actresses), a ilu reżyserami (director).\"\n",
    "\n",
    "# Wczytanie danych z datasource4 \n",
    "rdd4_raw = sc.textFile(datasource4_dir + \"/*.tsv\")\n",
    "header4 = rdd4_raw.first()\n",
    "rdd4_data = rdd4_raw.filter(lambda row: row != header4)\n",
    "\n",
    "def parse_person(line):\n",
    "    # Pola: nconst, primaryName, birthYear, deathYear, primaryProfession, knownForTitles\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) < 6:\n",
    "        fields += [\"\"]*(6-len(fields))\n",
    "    return (\n",
    "        fields[0],  # nconst\n",
    "        fields[1],  # primaryName\n",
    "        fields[2],  # birthYear\n",
    "        fields[3],  # deathYear\n",
    "        fields[4],  # primaryProfession\n",
    "        fields[5]   # knownForTitles\n",
    "    )\n",
    "\n",
    "# Zestaw wartości, które interpretujemy jako „aktor”\n",
    "ACTOR_VALUES = {\"actor\", \"actress\", \"actors\", \"actresses\"}\n",
    "\n",
    "def is_valid_year(yr):\n",
    "    return yr.isdigit()\n",
    "\n",
    "def valid_birth_and_death(rec):\n",
    "    \"\"\"\n",
    "    Zwraca True, jeśli birthYear i deathYear są liczbami oraz birthYear <= deathYear\n",
    "    \"\"\"\n",
    "    bYear = rec[2]\n",
    "    dYear = rec[3]\n",
    "    if not (bYear.isdigit() and dYear.isdigit()):\n",
    "        return False\n",
    "    return int(bYear) <= int(dYear)\n",
    "\n",
    "def is_actor(prof):\n",
    "    \"\"\"\n",
    "    Zwraca True, jeśli w liście profesji występuje jedno ze słów\n",
    "    (actor, actors, actress, actresses).\n",
    "    \"\"\"\n",
    "    prof_list = [p.strip() for p in prof.split(\",\")]\n",
    "    return any(p in ACTOR_VALUES for p in prof_list)\n",
    "\n",
    "def is_director(prof):\n",
    "    \"\"\"\n",
    "    Zwraca True, jeśli w liście profesji występuje 'director'.\n",
    "    \"\"\"\n",
    "    prof_list = [p.strip() for p in prof.split(\",\")]\n",
    "    return \"director\" in prof_list\n",
    "\n",
    "# Tworzymy RDD z krotkami\n",
    "rdd4_parsed = rdd4_data.map(parse_person)\n",
    "\n",
    "# Filtrowanie: birthYear i deathYear są liczbami + birthYear <= deathYear\n",
    "rdd4_dead = rdd4_parsed.filter(valid_birth_and_death)\n",
    "\n",
    "def to_age_tuple(rec):\n",
    "    \"\"\"\n",
    "    Zwraca ((age), (count_person=1, isActor, isDirector)).\n",
    "    \"\"\"\n",
    "    nconst, primaryName, bYear, dYear, prof, knownFor = rec\n",
    "    age = int(dYear) - int(bYear)\n",
    "    a = 1 if is_actor(prof) else 0\n",
    "    d = 1 if is_director(prof) else 0\n",
    "    return (age, (1, a, d))\n",
    "\n",
    "# Mapujemy do postaci (age, (1, isActor, isDirector))\n",
    "rdd4_age = rdd4_dead.map(to_age_tuple)\n",
    "\n",
    "# Redukcja: sumujemy\n",
    "# => (age, (count_persons, count_actors, count_directors))\n",
    "rdd4_reduced = rdd4_age.reduceByKey(\n",
    "    lambda agg, val: (agg[0]+val[0], agg[1]+val[1], agg[2]+val[2])\n",
    ")\n",
    "\n",
    "# Sortowanie rosnąco po kluczu 'age'\n",
    "rdd4_sorted = rdd4_reduced.sortBy(lambda x: x[0])\n",
    "\n",
    "# Pobieramy do drivera i wyświetlamy\n",
    "results_m1_rdd = rdd4_sorted.collect()\n",
    "\n",
    "print(\"Wyniki Misja 1 (RDD):\")\n",
    "print(\"age | persons | actors | directors\")\n",
    "for (age, (persons, actors, directors)) in results_m1_rdd:\n",
    "    print(f\"{age}\\t{persons}\\t{actors}\\t{directors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:====================================================>(104 + 1) / 105]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki Misja 2 (RDD):\n",
      "primaryName | birthYear | age | filmCount | filmCountAsActor | filmCountAsDirector\n",
      "('Reg Watson', '1926', 93, 13408, 1, 184)\n",
      "('Tony Warren', '1937', 79, 9874, 9, 0)\n",
      "('William J. Bell', '1927', 78, 8483, 4, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 2 - RDD)\n",
    "#------------\n",
    "# Zadanie:\n",
    "# \"Wśród osób, które urodziły się w ubiegłym wieku (np. 1900–1999) i przeżyły ponad 70 lat,\n",
    "#  wyznacz te trzy, które brały udział w największej liczbie filmów. \n",
    "#  Określ też w ilu filmach byli oni aktorami (role in [actor, actress, self]) \n",
    "#  oraz ile filmów reżyserowali (role = director).\"\n",
    "\n",
    "\n",
    "#rdd1_raw = sc.textFile(datasource1_dir + \"/part-*\")\n",
    "rdd1_raw = sc.textFile(datasource1_dir + \"/*.tsv\")\n",
    "\n",
    "def parse_title_person(line):\n",
    "    # Pola: tconst, ordering, nconst, role, job, characters\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) < 6:\n",
    "        fields += [\"\"] * (6 - len(fields))\n",
    "    return (\n",
    "        fields[0],  # tconst\n",
    "        fields[1],  # ordering\n",
    "        fields[2],  # nconst\n",
    "        fields[3],  # role\n",
    "        fields[4],  # job\n",
    "        fields[5],  # characters\n",
    "    )\n",
    "\n",
    "rdd1_parsed = rdd1_raw.map(parse_title_person)\n",
    "\n",
    "# Wczytanie datasource4 (przyjmujemy, że w jednym z plików jest wiersz nagłówkowy)\n",
    "rdd4_raw = sc.textFile(datasource4_dir + \"/name.basics.tsv\")\n",
    "\n",
    "# Pobieramy pierwszy wiersz jako nagłówek...\n",
    "header4 = rdd4_raw.first()\n",
    "# ...i filtrujemy go (dotyczy wyłącznie tego wiersza, w razie potrzeby można pominąć)\n",
    "rdd4_data = rdd4_raw.filter(lambda row: row != header4)\n",
    "\n",
    "def parse_person(line):\n",
    "    # Pola: nconst, primaryName, birthYear, deathYear, primaryProfession, knownForTitles\n",
    "    fields = line.split(\"\\t\")\n",
    "    if len(fields) < 6:\n",
    "        fields += [\"\"] * (6 - len(fields))\n",
    "    return (\n",
    "        fields[0],  # nconst\n",
    "        fields[1],  # primaryName\n",
    "        fields[2],  # birthYear\n",
    "        fields[3],  # deathYear\n",
    "        fields[4],  # primaryProfession\n",
    "        fields[5],  # knownForTitles\n",
    "    )\n",
    "\n",
    "def is_valid_year(yr):\n",
    "    return yr.isdigit()\n",
    "\n",
    "# Konwersja do RDD obiektów\n",
    "rdd4_parsed = rdd4_data.map(parse_person)\n",
    "\n",
    "# Filtr: birthYear w [1900..1999], deathYear liczba, i (deathYear - birthYear) > 70\n",
    "def filter_ubiegly_wiek_over70(rec):\n",
    "    nconst, name, bYear, dYear, profs, known = rec\n",
    "    if not (is_valid_year(bYear) and is_valid_year(dYear)):\n",
    "        return False\n",
    "    by = int(bYear)\n",
    "    dy = int(dYear)\n",
    "    age = dy - by\n",
    "    return (1900 <= by <= 1999) and (age > 70)\n",
    "\n",
    "rdd4_filtered = rdd4_parsed.filter(filter_ubiegly_wiek_over70)\n",
    "\n",
    "# Mapa: (nconst, (primaryName, birthYear, age))\n",
    "def map_nconst_info(rec):\n",
    "    nconst, name, bYear, dYear, profs, known = rec\n",
    "    return (\n",
    "        nconst,\n",
    "        (name, bYear, int(dYear) - int(bYear))\n",
    "    )\n",
    "\n",
    "rdd4_prepared = rdd4_filtered.map(map_nconst_info)\n",
    "\n",
    "# Utworzenie RDD (nconst, (tconst, role)) z datasource1\n",
    "rdd1_mapped = rdd1_parsed.map(lambda x: (x[2], (x[0], x[3])))\n",
    "\n",
    "# Łączenie po kluczu nconst\n",
    "joined_rdd = rdd4_prepared.join(rdd1_mapped)\n",
    "# => (nconst, ((primaryName, birthYear, age), (tconst, role)))\n",
    "\n",
    "# Grupowanie po nconst\n",
    "grouped_rdd = joined_rdd.groupByKey()\n",
    "\n",
    "def aggregate_person_data(records):\n",
    "    # records to zbiór: [((primaryName, birthYear, age), (tconst, role)), ...]\n",
    "    all_movies = set()\n",
    "    actor_movies = set()\n",
    "    director_movies = set()\n",
    "    primaryName = None\n",
    "    bYear = None\n",
    "    age = None\n",
    "\n",
    "    for elem in records:\n",
    "        (pName, bYearTmp, aTmp) = elem[0]  # (primaryName, birthYear, age)\n",
    "        (tconst, role) = elem[1]          # (tconst, role)\n",
    "\n",
    "        if primaryName is None:\n",
    "            primaryName = pName\n",
    "            bYear = bYearTmp\n",
    "            age = aTmp\n",
    "\n",
    "        all_movies.add(tconst)\n",
    "        # Rola w [actor, actress, self] => liczymy jako aktora\n",
    "        if role in [\"actor\", \"actress\", \"self\"]:\n",
    "            actor_movies.add(tconst)\n",
    "        if role == \"director\":\n",
    "            director_movies.add(tconst)\n",
    "\n",
    "    return (\n",
    "        primaryName,\n",
    "        bYear,\n",
    "        age,\n",
    "        len(all_movies),\n",
    "        len(actor_movies),\n",
    "        len(director_movies)\n",
    "    )\n",
    "\n",
    "# (nconst, (primaryName, birthYear, age, filmCount, filmCountAsActor, filmCountAsDirector))\n",
    "aggregated = grouped_rdd.mapValues(aggregate_person_data)\n",
    "\n",
    "# Interesują nas wyłącznie wartości, bez klucza nconst\n",
    "res_m2_rdd = aggregated.map(lambda x: x[1])\n",
    "\n",
    "# Sortowanie malejąco wg filmCount (indeks 3 w krotce)\n",
    "sorted_m2 = res_m2_rdd.sortBy(lambda x: x[3], ascending=False)\n",
    "\n",
    "# Wyciągnięcie top3\n",
    "top3_m2_rdd = sorted_m2.take(3)\n",
    "\n",
    "print(\"Wyniki Misja 2 (RDD):\")\n",
    "print(\"primaryName | birthYear | age | filmCount | filmCountAsActor | filmCountAsDirector\")\n",
    "for row in top3_m2_rdd:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d303a72b-4083-470e-b25d-3224360ee94f",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037689d7-f0ee-4165-bef0-83fa7f3e8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "before_rdd_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23971c0-cec7-4ea8-befb-7f063dce863c",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* oparte na *RDD API*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *RDD API* tego wymaga. \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie plików tekstowych\n",
    "text_files = sc.textFile(datasource4_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7955a5f-386d-47a5-9f6a-3d93a906c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział linii na słowa i zliczanie ilości wystąpień każdego słowa\n",
    "word_counts = text_files.flatMap(lambda line: line.split(\" \")) \\\n",
    "                        .map(lambda word: (word, 1)) \\\n",
    "                        .reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d77fd7-1f15-4365-ae80-c902aeb55ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis wyniku do pliku pickle\n",
    "word_counts.saveAsPickleFile(rdd_result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8b5ec-b799-4177-8e4a-80a583d995e7",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4325d378-b145-4e8f-8d37-80a072b506c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "after_rdd_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28137d3d-6f0d-443f-97b8-38104aaced6d",
   "metadata": {},
   "source": [
    "# Część 2 - Spark SQL (DataFrame)\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d045dae-5826-4015-8833-564d356db1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---------+\n",
      "|age|persons|actors|directors|\n",
      "+---+-------+------+---------+\n",
      "|0  |7      |2     |0        |\n",
      "|1  |5      |1     |0        |\n",
      "|2  |3      |1     |0        |\n",
      "|3  |5      |1     |0        |\n",
      "|4  |6      |2     |0        |\n",
      "|5  |4      |2     |0        |\n",
      "|6  |11     |3     |0        |\n",
      "|7  |10     |4     |0        |\n",
      "|8  |7      |4     |0        |\n",
      "|9  |10     |4     |0        |\n",
      "|10 |17     |10    |0        |\n",
      "|11 |15     |11    |0        |\n",
      "|12 |16     |8     |0        |\n",
      "|13 |22     |15    |0        |\n",
      "|14 |38     |15    |0        |\n",
      "|15 |32     |15    |1        |\n",
      "|16 |41     |26    |1        |\n",
      "|17 |50     |29    |2        |\n",
      "|18 |71     |38    |1        |\n",
      "|19 |86     |53    |1        |\n",
      "|20 |115    |79    |2        |\n",
      "|21 |103    |73    |2        |\n",
      "|22 |149    |100   |3        |\n",
      "|23 |160    |87    |5        |\n",
      "|24 |199    |116   |9        |\n",
      "|25 |219    |136   |4        |\n",
      "|26 |265    |153   |12       |\n",
      "|27 |304    |185   |7        |\n",
      "|28 |343    |206   |15       |\n",
      "|29 |352    |207   |18       |\n",
      "|30 |390    |235   |23       |\n",
      "|31 |408    |259   |31       |\n",
      "|32 |421    |271   |31       |\n",
      "|33 |438    |271   |29       |\n",
      "|34 |558    |354   |31       |\n",
      "|35 |515    |304   |47       |\n",
      "|36 |547    |341   |53       |\n",
      "|37 |622    |366   |53       |\n",
      "|38 |645    |368   |61       |\n",
      "|39 |721    |436   |61       |\n",
      "|40 |746    |448   |72       |\n",
      "|41 |776    |454   |91       |\n",
      "|42 |754    |449   |78       |\n",
      "|43 |867    |526   |100      |\n",
      "|44 |937    |595   |92       |\n",
      "|45 |989    |572   |106      |\n",
      "|46 |1099   |643   |123      |\n",
      "|47 |1082   |628   |118      |\n",
      "|48 |1240   |720   |168      |\n",
      "|49 |1338   |796   |142      |\n",
      "|50 |1458   |839   |185      |\n",
      "|51 |1462   |858   |181      |\n",
      "|52 |1577   |961   |163      |\n",
      "|53 |1675   |953   |189      |\n",
      "|54 |1746   |1022  |201      |\n",
      "|55 |1845   |1109  |202      |\n",
      "|56 |1973   |1158  |229      |\n",
      "|57 |2072   |1213  |234      |\n",
      "|58 |2275   |1334  |273      |\n",
      "|59 |2354   |1364  |258      |\n",
      "|60 |2420   |1400  |271      |\n",
      "|61 |2603   |1563  |280      |\n",
      "|62 |2670   |1561  |299      |\n",
      "|63 |2862   |1689  |329      |\n",
      "|64 |2928   |1727  |332      |\n",
      "|65 |3113   |1867  |370      |\n",
      "|66 |3230   |1909  |347      |\n",
      "|67 |3419   |2031  |375      |\n",
      "|68 |3482   |2050  |404      |\n",
      "|69 |3623   |2148  |439      |\n",
      "|70 |3720   |2254  |403      |\n",
      "|71 |3743   |2228  |415      |\n",
      "|72 |3870   |2330  |440      |\n",
      "|73 |3937   |2332  |451      |\n",
      "|74 |4140   |2469  |447      |\n",
      "|75 |4308   |2507  |450      |\n",
      "|76 |4138   |2421  |458      |\n",
      "|77 |4154   |2394  |476      |\n",
      "|78 |4271   |2536  |454      |\n",
      "|79 |4134   |2390  |462      |\n",
      "|80 |4234   |2484  |445      |\n",
      "|81 |4213   |2461  |440      |\n",
      "|82 |4214   |2425  |428      |\n",
      "|83 |4109   |2318  |417      |\n",
      "|84 |4007   |2271  |437      |\n",
      "|85 |3896   |2210  |413      |\n",
      "|86 |3764   |2081  |406      |\n",
      "|87 |3486   |1967  |362      |\n",
      "|88 |3342   |1827  |322      |\n",
      "|89 |3124   |1671  |305      |\n",
      "|90 |2867   |1542  |290      |\n",
      "|91 |2438   |1309  |209      |\n",
      "|92 |2235   |1206  |184      |\n",
      "|93 |1911   |1006  |162      |\n",
      "|94 |1587   |826   |130      |\n",
      "|95 |1316   |648   |109      |\n",
      "|96 |1075   |546   |78       |\n",
      "|97 |820    |374   |57       |\n",
      "|98 |618    |306   |54       |\n",
      "|99 |467    |206   |28       |\n",
      "|100|316    |152   |20       |\n",
      "|101|268    |125   |20       |\n",
      "|102|179    |79    |11       |\n",
      "|103|121    |57    |2        |\n",
      "|104|87     |34    |6        |\n",
      "|105|51     |19    |3        |\n",
      "|106|46     |19    |3        |\n",
      "|107|18     |4     |1        |\n",
      "|108|21     |4     |2        |\n",
      "|109|7      |2     |0        |\n",
      "|110|7      |2     |0        |\n",
      "|111|5      |1     |0        |\n",
      "|112|3      |1     |0        |\n",
      "|113|2      |0     |0        |\n",
      "|114|1      |0     |0        |\n",
      "|115|3      |0     |0        |\n",
      "|116|2      |0     |0        |\n",
      "|117|1      |1     |0        |\n",
      "|118|1      |0     |0        |\n",
      "|121|1      |1     |0        |\n",
      "|122|1      |0     |0        |\n",
      "+---+-------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 1 - DataFrame)\n",
    "#------------\n",
    "from pyspark.sql.functions import col, when, split, array_contains, size, count, sum\n",
    "\n",
    "# Zestaw wartości, które uznajemy za 'aktor'\n",
    "ACTOR_VALUES = [\"actor\",\"actress\",\"actors\",\"actresses\"]\n",
    "\n",
    "# Wczytujemy datasource4 z nagłówkiem (format TSV)\n",
    "df4 = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .csv(datasource4_dir + \"/*.tsv\"))\n",
    "\n",
    "# Filtr: birthYear i deathYear są liczbami + birthYear <= deathYear\n",
    "df4_valid = df4.filter(\n",
    "    (col(\"birthYear\").rlike(\"^[0-9]+$\")) &\n",
    "    (col(\"deathYear\").rlike(\"^[0-9]+$\")) &\n",
    "    (col(\"birthYear\").cast(\"int\") <= col(\"deathYear\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Konwersja kolumn na integer\n",
    "df4_casted = df4_valid.withColumn(\"birthYear\", col(\"birthYear\").cast(\"int\")) \\\n",
    "                     .withColumn(\"deathYear\", col(\"deathYear\").cast(\"int\"))\n",
    "\n",
    "# Wyliczamy age\n",
    "df4_age = df4_casted.withColumn(\"age\", col(\"deathYear\") - col(\"birthYear\"))\n",
    "\n",
    "# Definiujemy kolumny isActor / isDirector\n",
    "# isActor = 1, jeśli w polu primaryProfession występuje (actor, actress, actors, actresses)\n",
    "# isDirector = 1, jeśli w polu primaryProfession występuje 'director'\n",
    "df4_age = df4_age.withColumn(\n",
    "    \"isActor\",\n",
    "    when(\n",
    "        # array_contains(..., ...) => sprawdza, czy dany element jest w tablicy\n",
    "        # aby uwzględnić wiele wartości, łączymy je warunkami OR\n",
    "        (array_contains(split(col(\"primaryProfession\"), \",\"), ACTOR_VALUES[0]))\n",
    "        | (array_contains(split(col(\"primaryProfession\"), \",\"), ACTOR_VALUES[1]))\n",
    "        | (array_contains(split(col(\"primaryProfession\"), \",\"), ACTOR_VALUES[2]))\n",
    "        | (array_contains(split(col(\"primaryProfession\"), \",\"), ACTOR_VALUES[3])),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "df4_age = df4_age.withColumn(\n",
    "    \"isDirector\",\n",
    "    when(\n",
    "        array_contains(split(col(\"primaryProfession\"), \",\"), \"director\"), 1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Grupowanie po age\n",
    "df_agg = df4_age.groupBy(\"age\").agg(\n",
    "    count(\"*\").alias(\"persons\"),\n",
    "    sum(\"isActor\").alias(\"actors\"),\n",
    "    sum(\"isDirector\").alias(\"directors\")\n",
    ")\n",
    "\n",
    "df_agg.orderBy(\"age\").show(200, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7738406-c426-4238-b0fb-983f4585bc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 128:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---+---------+----------------+-------------------+\n",
      "|primaryName    |birthYear|age|filmCount|filmCountAsActor|filmCountAsDirector|\n",
      "+---------------+---------+---+---------+----------------+-------------------+\n",
      "|Reg Watson     |1926     |93 |13408    |1               |184                |\n",
      "|Tony Warren    |1937     |79 |9874     |9               |0                  |\n",
      "|William J. Bell|1927     |78 |8483     |4               |0                  |\n",
      "+---------------+---------+---+---------+----------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 2 - DataFrame)\n",
    "#------------\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, explode, expr, array_contains, size,\n",
    "    count, sum, collect_set\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# 1) DataFrame z datasource4 (pliki .tsv, z nagłówkiem)\n",
    "df4 = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .csv(datasource4_dir + \"/*.tsv\"))  # lub ... + \"/name.basics.tsv\"\n",
    "\n",
    "df4 = df4.filter(\n",
    "    (col(\"birthYear\").rlike(\"^[0-9]+$\")) &\n",
    "    (col(\"deathYear\").rlike(\"^[0-9]+$\"))\n",
    ").withColumn(\"birthYear\", col(\"birthYear\").cast(\"int\")) \\\n",
    " .withColumn(\"deathYear\", col(\"deathYear\").cast(\"int\"))\n",
    "\n",
    "# Dodajemy kolumnę age\n",
    "df4 = df4.withColumn(\"age\", col(\"deathYear\") - col(\"birthYear\"))\n",
    "\n",
    "# Filtr: urodzeni w [1900..1999], żyli > 70 lat\n",
    "df4_filtr = df4.filter(\n",
    "    (col(\"birthYear\") >= 1900) &\n",
    "    (col(\"birthYear\") <= 1999) &\n",
    "    (col(\"age\") > 70)\n",
    ")\n",
    "\n",
    "# 2) DataFrame z datasource1 (brak nagłówka, pliki part-xxxx)\n",
    "schema_ds1 = StructType([\n",
    "    StructField(\"tconst\", StringType(), True),\n",
    "    StructField(\"ordering\", StringType(), True),\n",
    "    StructField(\"nconst\", StringType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"characters\", StringType(), True)\n",
    "])\n",
    "\n",
    "df1 = (spark.read\n",
    "    .option(\"header\", False)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .schema(schema_ds1)\n",
    "    .csv(datasource1_dir + \"/*.tsv\"))\n",
    "#df1 = (spark.read\n",
    "#    .option(\"header\", False)    \n",
    "#    .option(\"sep\", \"\\t\")\n",
    "#    .schema(schema_ds1)\n",
    "#    .csv(datasource1_dir + \"/part-*\")) \n",
    "\n",
    "# JOIN po nconst\n",
    "df_join = df4_filtr.join(df1, on=\"nconst\", how=\"inner\")\n",
    "\n",
    "# Zliczamy liczbę unikalnych tconst, unikalnych tconst gdzie role in [actor, actress, self], role=director\n",
    "df_grouped = df_join.groupBy(\"nconst\", \"primaryName\", \"birthYear\", \"age\") \\\n",
    "    .agg(\n",
    "        collect_set(\"tconst\").alias(\"allMovies\"),\n",
    "        collect_set(expr(\"CASE WHEN role IN ('actor','actress','self') THEN tconst END\")).alias(\"actorMovies\"),\n",
    "        collect_set(expr(\"CASE WHEN role = 'director' THEN tconst END\")).alias(\"directorMovies\"),\n",
    "    )\n",
    "\n",
    "# Dodajemy kolumny z liczebnością\n",
    "df_final = df_grouped.select(\n",
    "    col(\"primaryName\"),\n",
    "    col(\"birthYear\"),\n",
    "    col(\"age\"),\n",
    "    size(col(\"allMovies\")).alias(\"filmCount\"),\n",
    "    size(col(\"actorMovies\")).alias(\"filmCountAsActor\"),\n",
    "    size(col(\"directorMovies\")).alias(\"filmCountAsDirector\")\n",
    ").orderBy(col(\"filmCount\").desc())\n",
    "\n",
    "# Wybieramy top3\n",
    "df_final.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e569f-5f6b-4a98-b177-1b6fb0fc3333",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6329c04b-3e50-41a8-93f1-333ac0ea64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "before_df_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** *misji głównej* swojego projektu oparte o *DataFrame API*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawić\". \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, count\n",
    "# Wczytanie danych\n",
    "data = spark.read.text(datasource4_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcc4aaa9-8dc2-4726-871e-5e2450ba3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dzielenie linii na słowa i eksplozja do osobnych wierszy\n",
    "words = data.select(explode(split(data.value, \" \")).alias(\"word\"))\n",
    "\n",
    "# Zliczanie słów\n",
    "word_counts = words.groupBy(\"word\").agg(count(\"word\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45165cca-5197-4590-ba69-7541085147f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis wyników do tabeli \n",
    "word_counts.write.mode(\"overwrite\").saveAsTable(df_result_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0797752-450e-4f8f-a1d4-93a890a62c3d",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3647eae-2801-46ac-b43d-74e5bbfcab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIE ZMIENIAĆ\n",
    "after_df_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed01aa-cc23-427e-84c8-e5b76b9323bb",
   "metadata": {},
   "source": [
    "# Część 3 - Pandas API on Spark\n",
    "\n",
    "Ta część to wyzwanie. W szczególności dla osób, które nie programują na co dzień w Pythonie, lub które nie nie korzystały do tej pory z Pandas API.  \n",
    "\n",
    "Powodzenia!\n",
    "\n",
    "## Misje poboczne\n",
    "\n",
    "W ponizszych paragrafach wprowadź swoje rozwiązania *misji pobocznych*, o ile **nie** chcesz, aby oceniana była *misja główna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "971a265f-db04-4a26-936d-18ab875ddffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:5723: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  warnings.warn(\n",
      "[Stage 109:=============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     persons  actors  directors\n",
      "age                            \n",
      "0          7       2          0\n",
      "1          5       1          0\n",
      "2          3       1          0\n",
      "3          5       1          0\n",
      "4          6       2          0\n",
      "5          4       2          0\n",
      "6         11       3          0\n",
      "7         10       4          0\n",
      "8          7       4          0\n",
      "9         10       4          0\n",
      "10        17      10          0\n",
      "11        15      11          0\n",
      "12        16       8          0\n",
      "13        22      15          0\n",
      "14        38      15          0\n",
      "15        32      15          1\n",
      "16        41      26          1\n",
      "17        50      29          2\n",
      "18        71      38          1\n",
      "19        86      53          1\n",
      "20       115      79          2\n",
      "21       103      73          2\n",
      "22       149     100          3\n",
      "23       160      87          5\n",
      "24       199     116          9\n",
      "25       219     136          4\n",
      "26       265     153         12\n",
      "27       304     185          7\n",
      "28       343     206         15\n",
      "29       352     207         18\n",
      "30       390     235         23\n",
      "31       408     259         31\n",
      "32       421     271         31\n",
      "33       438     271         29\n",
      "34       558     354         31\n",
      "35       515     304         47\n",
      "36       547     341         53\n",
      "37       622     366         53\n",
      "38       645     368         61\n",
      "39       721     436         61\n",
      "40       746     448         72\n",
      "41       776     454         91\n",
      "42       754     449         78\n",
      "43       867     526        100\n",
      "44       937     595         92\n",
      "45       989     572        106\n",
      "46      1099     643        123\n",
      "47      1082     628        118\n",
      "48      1240     720        168\n",
      "49      1338     796        142\n",
      "50      1458     839        185\n",
      "51      1462     858        181\n",
      "52      1577     961        163\n",
      "53      1675     953        189\n",
      "54      1746    1022        201\n",
      "55      1845    1109        202\n",
      "56      1973    1158        229\n",
      "57      2072    1213        234\n",
      "58      2275    1334        273\n",
      "59      2354    1364        258\n",
      "60      2420    1400        271\n",
      "61      2603    1563        280\n",
      "62      2670    1561        299\n",
      "63      2862    1689        329\n",
      "64      2928    1727        332\n",
      "65      3113    1867        370\n",
      "66      3230    1909        347\n",
      "67      3419    2031        375\n",
      "68      3482    2050        404\n",
      "69      3623    2148        439\n",
      "70      3720    2254        403\n",
      "71      3743    2228        415\n",
      "72      3870    2330        440\n",
      "73      3937    2332        451\n",
      "74      4140    2469        447\n",
      "75      4308    2507        450\n",
      "76      4138    2421        458\n",
      "77      4154    2394        476\n",
      "78      4271    2536        454\n",
      "79      4134    2390        462\n",
      "80      4234    2484        445\n",
      "81      4213    2461        440\n",
      "82      4214    2425        428\n",
      "83      4109    2318        417\n",
      "84      4007    2271        437\n",
      "85      3896    2210        413\n",
      "86      3764    2081        406\n",
      "87      3486    1967        362\n",
      "88      3342    1827        322\n",
      "89      3124    1671        305\n",
      "90      2867    1542        290\n",
      "91      2438    1309        209\n",
      "92      2235    1206        184\n",
      "93      1911    1006        162\n",
      "94      1587     826        130\n",
      "95      1316     648        109\n",
      "96      1075     546         78\n",
      "97       820     374         57\n",
      "98       618     306         54\n",
      "99       467     206         28\n",
      "100      316     152         20\n",
      "101      268     125         20\n",
      "102      179      79         11\n",
      "103      121      57          2\n",
      "104       87      34          6\n",
      "105       51      19          3\n",
      "106       46      19          3\n",
      "107       18       4          1\n",
      "108       21       4          2\n",
      "109        7       2          0\n",
      "110        7       2          0\n",
      "111        5       1          0\n",
      "112        3       1          0\n",
      "113        2       0          0\n",
      "114        1       0          0\n",
      "115        3       0          0\n",
      "116        2       0          0\n",
      "117        1       1          0\n",
      "118        1       0          0\n",
      "121        1       1          0\n",
      "122        1       0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/25 15:03:04 WARN AttachDistributedSequenceExec: clean up cached RDD(323) in AttachDistributedSequenceExec(1550)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 1 - Pandas API on Spark)\n",
    "#------------\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Zestaw wartości, które uznajemy za 'aktor'\n",
    "ACTOR_VALUES = {\"actor\",\"actress\",\"actors\",\"actresses\"}\n",
    "\n",
    "# Wczytujemy datasource4 z nagłówkiem do Spark DataFrame\n",
    "df4_spark = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .csv(datasource4_dir + \"/*.tsv\")\n",
    ")\n",
    "\n",
    "# Konwersja do ps.DataFrame (Pandas API on Spark)\n",
    "df4_pdf = df4_spark.to_pandas_on_spark()\n",
    "\n",
    "# 1) Filtr: birthYear, deathYear są liczbami (^\\d+$) i birthYear <= deathYear\n",
    "df4_pdf = df4_pdf[\n",
    "    df4_pdf[\"birthYear\"].str.match(r\"^\\d+$\") &\n",
    "    df4_pdf[\"deathYear\"].str.match(r\"^\\d+$\")\n",
    "]\n",
    "# Konwertujemy na int\n",
    "df4_pdf[\"birthYear\"] = df4_pdf[\"birthYear\"].astype(int)\n",
    "df4_pdf[\"deathYear\"] = df4_pdf[\"deathYear\"].astype(int)\n",
    "\n",
    "df4_pdf = df4_pdf[df4_pdf[\"birthYear\"] <= df4_pdf[\"deathYear\"]]\n",
    "\n",
    "# 2) Obliczamy age\n",
    "df4_pdf[\"age\"] = df4_pdf[\"deathYear\"] - df4_pdf[\"birthYear\"]\n",
    "\n",
    "# Funkcje pomocnicze do ustalania isActor, isDirector\n",
    "def check_actor(profs: str) -> int:\n",
    "    if not isinstance(profs, str):\n",
    "        return 0\n",
    "    splitted = [p.strip() for p in profs.split(\",\")]\n",
    "    # Zwraca 1, jeśli w splitted pojawia się co najmniej jeden element z ACTOR_VALUES\n",
    "    return 1 if any(p in ACTOR_VALUES for p in splitted) else 0\n",
    "\n",
    "def check_director(profs: str) -> int:\n",
    "    if not isinstance(profs, str):\n",
    "        return 0\n",
    "    splitted = [p.strip() for p in profs.split(\",\")]\n",
    "    return 1 if \"director\" in splitted else 0\n",
    "\n",
    "df4_pdf[\"isActor\"] = df4_pdf[\"primaryProfession\"].apply(check_actor)\n",
    "df4_pdf[\"isDirector\"] = df4_pdf[\"primaryProfession\"].apply(check_director)\n",
    "\n",
    "# 3) Grupujemy po age: zliczamy persons, sumę isActor, sumę isDirector\n",
    "grouped = df4_pdf.groupby(\"age\").agg(\n",
    "    persons=(\"nconst\", \"count\"),\n",
    "    actors=(\"isActor\", \"sum\"),\n",
    "    directors=(\"isDirector\", \"sum\")\n",
    ").sort_index()\n",
    "\n",
    "print(grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "91621654-a24e-4ddb-b2c7-9f149252af13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:5723: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/spark/python/pyspark/sql/dataframe.py:5723: FutureWarning: DataFrame.to_pandas_on_spark is deprecated. Use DataFrame.pandas_api instead.\n",
      "  warnings.warn(\n",
      "24/12/25 15:23:12 WARN AttachDistributedSequenceExec: clean up cached RDD(399) in AttachDistributedSequenceExec(1910)\n",
      "/usr/lib/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If the type hints is not specified for `groupby.apply`, it is expensive to infer the data type internally.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "24/12/25 15:24:24 WARN AttachDistributedSequenceExec: clean up cached RDD(427) in AttachDistributedSequenceExec(2136)\n",
      "24/12/25 15:26:01 WARN AttachDistributedSequenceExec: clean up cached RDD(458) in AttachDistributedSequenceExec(2413)\n",
      "24/12/25 15:27:01 WARN AttachDistributedSequenceExec: clean up cached RDD(474) in AttachDistributedSequenceExec(2729)\n",
      "[Stage 164:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          nconst      primaryName  birthYear  age  filmCount  filmCountAsActor  filmCountAsDirector\n",
      "51059  nm0914844       Reg Watson       1926   93      13408                 1                  184\n",
      "51043  nm0912726      Tony Warren       1937   79       9874                 9                    0\n",
      "878    nm0068589  William J. Bell       1927   78       8483                 4                    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#------------\n",
    "# KOD (Misja 2 - Pandas API on Spark)\n",
    "#------------\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# 1) Wczytanie danych z datasource4 do Spark DF, konwersja do ps.DataFrame\n",
    "df4_spark = (spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .csv(datasource4_dir + \"/*.tsv\"))\n",
    "\n",
    "df4_pdf = df4_spark.to_pandas_on_spark()\n",
    "\n",
    "df4_pdf = df4_pdf[\n",
    "    (df4_pdf[\"birthYear\"].str.match(r\"^\\d+$\")) &\n",
    "    (df4_pdf[\"deathYear\"].str.match(r\"^\\d+$\"))\n",
    "]\n",
    "df4_pdf[\"birthYear\"] = df4_pdf[\"birthYear\"].astype(int)\n",
    "df4_pdf[\"deathYear\"] = df4_pdf[\"deathYear\"].astype(int)\n",
    "df4_pdf[\"age\"] = df4_pdf[\"deathYear\"] - df4_pdf[\"birthYear\"]\n",
    "\n",
    "df4_pdf = df4_pdf[\n",
    "    (df4_pdf[\"birthYear\"] >= 1900) &\n",
    "    (df4_pdf[\"birthYear\"] <= 1999) &\n",
    "    (df4_pdf[\"age\"] > 70)\n",
    "]\n",
    "\n",
    "# 2) Wczytanie datasource1\n",
    "schema_ds1 = StructType([\n",
    "    StructField(\"tconst\", StringType(), True),\n",
    "    StructField(\"ordering\", StringType(), True),\n",
    "    StructField(\"nconst\", StringType(), True),\n",
    "    StructField(\"role\", StringType(), True),\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"characters\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df1_spark = (spark.read\n",
    "    .option(\"header\", False)\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .schema(schema_ds1)\n",
    "    .csv(datasource1_dir + \"/*.tsv\"))\n",
    "#df1_spark = (spark.read\n",
    "#    .option(\"header\", False)  \n",
    "#    .option(\"sep\", \"\\t\")\n",
    "#    .schema(schema_ds1)\n",
    "#    .csv(datasource1_dir + \"/part-*\"))\n",
    "\n",
    "\n",
    "df1_pdf = df1_spark.to_pandas_on_spark()\n",
    "\n",
    "# Join po nconst\n",
    "joined_pdf = df4_pdf.merge(df1_pdf, on=\"nconst\", how=\"inner\")\n",
    "\n",
    "# Teraz musimy zgrupować po nconst i policzyć:\n",
    "# - liczbę unikalnych tconst,\n",
    "# - liczbę unikalnych tconst, gdzie role in [actor, actress, self],\n",
    "# - liczbę unikalnych tconst, gdzie role='director'.\n",
    "def role_map(role):\n",
    "    if role in [\"actor\",\"actress\",\"self\"]:\n",
    "        return \"actor\"\n",
    "    elif role == \"director\":\n",
    "        return \"director\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "joined_pdf[\"roleMapped\"] = joined_pdf[\"role\"].apply(role_map)\n",
    "\n",
    "grouped_pdf = joined_pdf.groupby([\"nconst\",\"primaryName\",\"birthYear\",\"age\"])\n",
    "\n",
    "def aggregate_films(df):\n",
    "    all_movies = set(df[\"tconst\"].values.tolist())\n",
    "    actor_movies = set(df[df[\"roleMapped\"]==\"actor\"][\"tconst\"].values.tolist())\n",
    "    director_movies = set(df[df[\"roleMapped\"]==\"director\"][\"tconst\"].values.tolist())\n",
    "    return pd.Series({\n",
    "        \"filmCount\": len(all_movies),\n",
    "        \"filmCountAsActor\": len(actor_movies),\n",
    "        \"filmCountAsDirector\": len(director_movies)\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "result_pdf = grouped_pdf.apply(aggregate_films).reset_index()\n",
    "\n",
    "# Sortujemy malejąco po filmCount i wyświetlamy top 3\n",
    "top3_pdf = result_pdf.sort_values(\"filmCount\", ascending=False).head(3)\n",
    "\n",
    "print(top3_pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5184ce-cf42-4342-aeec-b56c30b66bbd",
   "metadata": {},
   "source": [
    "## Misja główna \n",
    "\n",
    "Poniższy paragraf zapisuje metryki przed uruchomieniem Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63fd8306-87e9-46f2-b622-d60693e3ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIE ZMIENIAĆ\n",
    "before_ps_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967f079-7106-4bd7-9d26-98ced2aeb43b",
   "metadata": {},
   "source": [
    "W poniższych paragrafach wprowadź **rozwiązanie** swojego projektu oparte o *Pandas API on Spark*. \n",
    "\n",
    "Pamiętaj o wydajności Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawić\". \n",
    "\n",
    "Nie wprowadzaj w poniższych paragrafach żadnego kodu, w przypadku wykorzystania *misji pobocznych*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2094a69-30b1-4970-825b-2b0624436cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n",
      "/usr/local/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "lines_ps = ps.read_csv(datasource4_dir, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea69e909-a557-4294-b1ae-f0d551649eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_ps = lines_ps[0].apply(lambda x: x.split(' ') if x is not None else []).explode().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5759f50-b92e-41a5-9eb0-9b00e2528ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_counts = words_ps.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02917f4-e1f2-4fb4-8b53-8829fb3f0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas Series is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "word_counts_pandas = word_counts.head(50).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76e0d7f7-82f3-41d4-8267-cf288f2f6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_pandas.to_json(ps_result_file, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a0ec5-ab13-4e39-a572-e7adf8b8556a",
   "metadata": {},
   "source": [
    "Poniższy paragraf zapisuje metryki po uruchomieniu Twojego rozwiązania *misji głównej*. \n",
    "\n",
    "Nie musisz go uruchamiać podczas implementacji rozwiązania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "108bee2a-a847-4625-8e4a-939951ac9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NIE ZMIENIAĆ\n",
    "after_ps_metrics = get_current_metrics(spark_ui_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e266b-b5cd-41d0-aeab-c1edc365910d",
   "metadata": {},
   "source": [
    "# Analiza wyników i wydajności *misji głównych*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b67111-62d0-4657-b158-1ed37db9ed96",
   "metadata": {},
   "source": [
    "## Część 1 - Spark Core (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cfc9900-7e0c-49ff-adba-e339f83ffe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TeriyakiApps\\x01~\\x01teriyakiapps@gmail.com\\x018589934594', 1)\n",
      "('Koza\\x01http://www.xynapse.pl\\x01xynapse@xynapse.pl\\x018589934595', 1)\n",
      "('Tools\\x01https://vcb30cb43.app-ads-txt.com/app-ads.txt\\x01androtools222@gmail.com\\x018589934596', 1)\n",
      "('Muslim', 110)\n",
      "('FireFlies', 2)\n",
      "('Studio\\x01~\\x01manuariza95@gmail.com\\x018589934602', 1)\n",
      "('News', 494)\n",
      "('IST-Development\\x01https://istanbulit.com\\x01info@istanbulit.com\\x018589934604', 1)\n",
      "('FAStuidoTI\\x01~\\x01karimkhalfy@gmail.com\\x018589934605', 1)\n",
      "('Web4Minds,', 1)\n",
      "('V3', 8)\n",
      "('Smart', 2437)\n",
      "('Ltd\\x01http://www.v3smarttech.com\\x01support@v3smarttech.com\\x018589934607', 1)\n",
      "('Mobil', 143)\n",
      "('UNDERSCORE:', 1)\n",
      "('Apps', 6350)\n",
      "('and', 5289)\n",
      "('Games\\x01~\\x01ergamesapps@gmail.com\\x018589934609', 1)\n",
      "('tamapps\\x01~\\x01zakdermeister@gmail.com\\x018589934614', 1)\n",
      "('S.', 397)\n",
      "('Connect', 331)\n",
      "('Team\\x01https://mewe.com/join/klwpdevelopersteam\\x01designcorpviti@gmail.com\\x018589934618', 1)\n",
      "('for', 2565)\n",
      "('with', 262)\n",
      "('NETWORKS', 23)\n",
      "('PTE', 226)\n",
      "('Art\\x01https://www.bytesart.site\\x01support@bytesart.tech\\x018589934624', 1)\n",
      "('Mother', 28)\n",
      "('ShowMeTheParts\\x01http://www.ShowMeTheParts.com\\x01showmetheparts@gmail.com\\x018589934628', 1)\n",
      "('Sitevenia\\x01http://www.wmphotos.fr\\x01williammoureaux@sfr.fr\\x018589934629', 1)\n",
      "('NOVATIVE\\x01https://www.novative.com/\\x01sales@novative.com\\x018589934634', 1)\n",
      "('Chokurei', 5)\n",
      "('everyone\\x01https://www.sistemaeducativofinanciero.com/p/privacy\\x01sanz112358@gmail.com\\x018589934635', 1)\n",
      "('Orotti', 1)\n",
      "('Apps\\x01http://www.orotti.com\\x01apps@orotti.com\\x018589934636', 1)\n",
      "('Rabbitz', 2)\n",
      "('Games\\x01~\\x01madrabbitzgames@gmail.com\\x018589934637', 1)\n",
      "('ParkerSoft\\x01~\\x01ianparker2007@yahoo.co.uk\\x018589934638', 1)\n",
      "('ISHAN', 5)\n",
      "('Bismania', 2)\n",
      "('Wei', 36)\n",
      "('Jie\\x01https://github.com/myluckynumbers/In-Between\\x01limweijie250@gmail.com\\x018589934644', 1)\n",
      "('Iraqi', 3)\n",
      "('Investment', 273)\n",
      "('MV', 59)\n",
      "('S/A\\x01http://www.mv.com.br\\x01inovacaomv@gmail.com\\x018589934646', 1)\n",
      "('Welfare', 43)\n",
      "('Gosa\\x01https://www.facebook.com/themexperia\\x01support@mkninc.ru\\x018589934648', 1)\n",
      "('Frillapps\\x01https://weedleapps.co.il/\\x01ozvi.inc@gmail.com\\x018589934651', 1)\n",
      "('Beansprites', 5)\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie wyników z pliku pickle\n",
    "word_counts = sc.pickleFile(rdd_result_dir)\n",
    "\n",
    "# Wyświetlenie 50 pierwszych elementów\n",
    "result_sample = word_counts.take(50)\n",
    "for item in result_sample:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16edae69-8062-4422-842f-d50bca0af9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numTasks': 6,\n",
       " 'numActiveTasks': 0,\n",
       " 'numCompleteTasks': 6,\n",
       " 'numFailedTasks': 0,\n",
       " 'numKilledTasks': 0,\n",
       " 'numCompletedIndices': 6,\n",
       " 'executorDeserializeTime': 763,\n",
       " 'executorDeserializeCpuTime': 288417800,\n",
       " 'executorRunTime': 52789,\n",
       " 'executorCpuTime': 3791290300,\n",
       " 'resultSize': 12143,\n",
       " 'jvmGcTime': 1808,\n",
       " 'resultSerializationTime': 19,\n",
       " 'memoryBytesSpilled': 0,\n",
       " 'diskBytesSpilled': 0,\n",
       " 'peakExecutionMemory': 0,\n",
       " 'inputBytes': 84276905,\n",
       " 'inputRecords': 1179547,\n",
       " 'outputBytes': 90624535,\n",
       " 'outputRecords': 14566,\n",
       " 'shuffleRemoteBlocksFetched': 0,\n",
       " 'shuffleLocalBlocksFetched': 9,\n",
       " 'shuffleFetchWaitTime': 0,\n",
       " 'shuffleRemoteBytesRead': 0,\n",
       " 'shuffleRemoteBytesReadToDisk': 0,\n",
       " 'shuffleLocalBytesRead': 49730906,\n",
       " 'shuffleReadBytes': 49730906,\n",
       " 'shuffleReadRecords': 228,\n",
       " 'shuffleWriteBytes': 49730906,\n",
       " 'shuffleWriteTime': 405851800,\n",
       " 'shuffleWriteRecords': 228}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4",
   "metadata": {},
   "source": [
    "## Część 2 - Spark SQL (DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b950a09d-045e-4143-a3cf-8ecc7c73ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+\n",
      "|                     word|count|\n",
      "+-------------------------+-----+\n",
      "|                      The| 9372|\n",
      "|                   Bidhee|    7|\n",
      "|                Solutions| 6041|\n",
      "|                   ArtAce|    2|\n",
      "|                  PuyTech|    1|\n",
      "|                   McLeod|  208|\n",
      "|                      RTV|   13|\n",
      "|     Software\u0001http://p...|    1|\n",
      "|紫荊雜誌社\u0001https://bau...|    1|\n",
      "|                  Bacilio|    2|\n",
      "|     Developer\u0001https:/...|    1|\n",
      "|     Software\u0001http://w...|    1|\n",
      "|                  Backend|   13|\n",
      "|하이퍼펌프\u0001~\u0001hyper.cho...|    1|\n",
      "|                    METRO|   21|\n",
      "|     ADBAND\u0001http://www...|    1|\n",
      "|                      Tcf|    1|\n",
      "|                      Pug|   12|\n",
      "|              Techologies|    4|\n",
      "|     Tourism\u0001https://t...|    1|\n",
      "|     Kinsale\u0001~\u0001gourmet...|    1|\n",
      "|     English\u0001https://w...|    1|\n",
      "|                    Darul|   10|\n",
      "|                       📱|    3|\n",
      "|                  Panipat|    2|\n",
      "|     Konyukhov\u0001http://...|    1|\n",
      "|                     Bold|   38|\n",
      "|     Developer\u0001http://...|    1|\n",
      "|     Advertising\u0001http:...|    1|\n",
      "|                 CÁNTABRO|    2|\n",
      "|     Consultores\u0001http:...|    1|\n",
      "|                     Amit|  106|\n",
      "|     CONTACTS\u0001http://w...|    1|\n",
      "|                    Jimmy|   78|\n",
      "|     applications\u0001~\u0001ph...|    1|\n",
      "|     Rechts\u0001https://ww...|    1|\n",
      "|     KetchapPro\u0001~\u0001ketc...|    1|\n",
      "|                     GIDA|    9|\n",
      "|     dev\u0001~\u0001radsdev@mai...|    1|\n",
      "|     Dipre\u0001~\u0001diomaris0...|    1|\n",
      "|                   Games:|   41|\n",
      "|                Beautiful|   62|\n",
      "|                      Jio|   22|\n",
      "|                   Phenix|   13|\n",
      "|     Apps\u0001https://seqa...|    1|\n",
      "|                    Qulam|    2|\n",
      "|     Games\u0001http://tinm...|    1|\n",
      "|     RedPACT\u0001~\u0001nancyak...|    1|\n",
      "|                     Coin|   79|\n",
      "|                 Smartify|    5|\n",
      "+-------------------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.table(df_result_table)\n",
    "\n",
    "# Wyświetlenie 50 pierwszych rekordów\n",
    "df.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f344ed9-94c1-4d79-b839-1839548d8c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numTasks': 12,\n",
       " 'numActiveTasks': 0,\n",
       " 'numCompleteTasks': 8,\n",
       " 'numFailedTasks': 0,\n",
       " 'numKilledTasks': 0,\n",
       " 'numCompletedIndices': 8,\n",
       " 'executorDeserializeTime': 1254,\n",
       " 'executorDeserializeCpuTime': 446474600,\n",
       " 'executorRunTime': 54900,\n",
       " 'executorCpuTime': 22626614900,\n",
       " 'resultSize': 36185,\n",
       " 'jvmGcTime': 2428,\n",
       " 'resultSerializationTime': 110,\n",
       " 'memoryBytesSpilled': 0,\n",
       " 'diskBytesSpilled': 0,\n",
       " 'peakExecutionMemory': 440400752,\n",
       " 'inputBytes': 84344235,\n",
       " 'inputRecords': 1179547,\n",
       " 'outputBytes': 50321941,\n",
       " 'outputRecords': 1456441,\n",
       " 'shuffleRemoteBlocksFetched': 0,\n",
       " 'shuffleLocalBlocksFetched': 16,\n",
       " 'shuffleFetchWaitTime': 0,\n",
       " 'shuffleRemoteBytesRead': 0,\n",
       " 'shuffleRemoteBytesReadToDisk': 0,\n",
       " 'shuffleLocalBytesRead': 63406957,\n",
       " 'shuffleReadBytes': 63406957,\n",
       " 'shuffleReadRecords': 1622698,\n",
       " 'shuffleWriteBytes': 63406957,\n",
       " 'shuffleWriteTime': 817119700,\n",
       " 'shuffleWriteRecords': 1622698}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtract_metrics(after_df_metrics, before_df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063b46c-579d-4775-ba3f-837708279ea2",
   "metadata": {},
   "source": [
    "## Część 3 - Pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab5e31a2-fd31-40ca-be7b-b20b13dc38a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"-\": 12048,\n",
      "  \"&\": 11714,\n",
      "  \"The\": 9329,\n",
      "  \"App\": 8068,\n",
      "  \"of\": 7933,\n",
      "  \"de\": 7396,\n",
      "  \"Technologies\": 7056,\n",
      "  \"Solutions\": 6880,\n",
      "  \"Software\": 6813,\n",
      "  \"Co.\": 6800,\n",
      "  \"Apps\": 6579,\n",
      "  \"Media\": 6462,\n",
      "  \"Digital\": 5836,\n",
      "  \"Pvt\": 5602,\n",
      "  \"Pvt.\": 5290,\n",
      "  \"Games\": 5044,\n",
      "  \"Technology\": 5009,\n",
      "  \"and\": 4989,\n",
      "  \"Mobile\": 4754,\n",
      "  \"Private\": 4196,\n",
      "  \"\": 3879,\n",
      "  \"Development\": 3868,\n",
      "  \"Group\": 3761,\n",
      "  \"IT\": 3665,\n",
      "  \"Services\": 3527,\n",
      "  \"Tech\": 3445,\n",
      "  \"Game\": 3333,\n",
      "  \"Bank\": 3127,\n",
      "  \"by\": 2831,\n",
      "  \"Systems\": 2778,\n",
      "  \"International\": 2580,\n",
      "  \"Global\": 2579,\n",
      "  \"Web\": 2563,\n",
      "  \"for\": 2536,\n",
      "  \"BH\": 2507,\n",
      "  \"Appswiz\": 2452,\n",
      "  \"Smart\": 2436,\n",
      "  \"Studio\": 2330,\n",
      "  \"Credit\": 2215,\n",
      "  \"Pty\": 2202,\n",
      "  \"Free\": 2186,\n",
      "  \"Business\": 1840,\n",
      "  \"Radio\": 1810,\n",
      "  \"New\": 1778,\n",
      "  \"Health\": 1710,\n",
      "  \"Company\": 1710,\n",
      "  \"Online\": 1629,\n",
      "  \"My\": 1539,\n",
      "  \"Church\": 1516,\n",
      "  \"Creative\": 1458\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Odczytaj zawartość pliku JSON\n",
    "with open(ps_result_file, 'r') as file:\n",
    "    json_content = json.load(file)\n",
    "\n",
    "# Wyświetl zawartość\n",
    "print(json.dumps(json_content, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32788c91-3f8e-4fb1-8afc-5eb00938e687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numTasks': 33,\n",
       " 'numActiveTasks': 0,\n",
       " 'numCompleteTasks': 25,\n",
       " 'numFailedTasks': 0,\n",
       " 'numKilledTasks': 0,\n",
       " 'numCompletedIndices': 25,\n",
       " 'executorDeserializeTime': 1838,\n",
       " 'executorDeserializeCpuTime': 440241100,\n",
       " 'executorRunTime': 166601,\n",
       " 'executorCpuTime': 55323279000,\n",
       " 'resultSize': 134363,\n",
       " 'jvmGcTime': 4753,\n",
       " 'resultSerializationTime': 123,\n",
       " 'memoryBytesSpilled': 0,\n",
       " 'diskBytesSpilled': 0,\n",
       " 'peakExecutionMemory': 427817888,\n",
       " 'inputBytes': 385819487,\n",
       " 'inputRecords': 5409845,\n",
       " 'outputBytes': 0,\n",
       " 'outputRecords': 0,\n",
       " 'shuffleRemoteBlocksFetched': 0,\n",
       " 'shuffleLocalBlocksFetched': 20,\n",
       " 'shuffleFetchWaitTime': 0,\n",
       " 'shuffleRemoteBytesRead': 0,\n",
       " 'shuffleRemoteBytesReadToDisk': 0,\n",
       " 'shuffleLocalBytesRead': 61239298,\n",
       " 'shuffleReadBytes': 61239298,\n",
       " 'shuffleReadRecords': 1573467,\n",
       " 'shuffleWriteBytes': 61239298,\n",
       " 'shuffleWriteTime': 1111152100,\n",
       " 'shuffleWriteRecords': 1573467}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtract_metrics(after_ps_metrics, before_ps_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
